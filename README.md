Explainable AI Dashboard

A Streamlit-based interactive dashboard for interpreting Machine Learning models using SHAP (SHapley Additive exPlanations).

ðŸ“Œ Features
	â€¢	Upload your own CSV datasets
	â€¢	Automatically detects classification or regression tasks
	â€¢	Trains a Random Forest model
	â€¢	Computes global feature importance with SHAP beeswarm plots
	â€¢	Visualizes local feature attribution using SHAP waterfall plots
	â€¢	Downloadable SHAP visualizations
	â€¢	Handles both numerical and categorical features

ðŸš€ Getting Started

1. Clone the repository
   git clone https://github.com/YOUR_USERNAME/explainable-ai-dashboard.git
cd explainable-ai-dashboard
2. Create and activate a virtual environment (optional)
   python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
4. Install dependencies
   pip install -r requirements.txt
5. Run the app
   streamlit run app.py

 Sample Dataset

A sample dataset (sample_regression_data.csv) is provided for testing. It includes both categorical and numerical features for regression tasks.

<img width="1728" alt="Screenshot 2025-04-07 at 2 01 51â€¯AM" src="https://github.com/user-attachments/assets/15be104f-11ee-46f2-b592-6f0726caf3d3" />

![image](https://github.com/user-attachments/assets/1c28c51c-b37c-4be4-9bce-fce23675485b)
![image](https://github.com/user-attachments/assets/97528c13-980e-4e6d-923c-e5c0a405c12b)


ðŸ”§ Tech Stack
	â€¢	Python
	â€¢	Streamlit
	â€¢	SHAP
	â€¢	scikit-learn
	â€¢	Matplotlib

ðŸ“ˆ Future Features
	â€¢	Support for XGBoost, LightGBM, and CatBoost models
	â€¢	Upload pre-trained models
	â€¢	Display model performance metrics (accuracy, RMSE)
	â€¢	Compare multiple models
